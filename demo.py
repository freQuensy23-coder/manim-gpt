# gradio_manim_gemini_app.py â€“ **v3**
"""Gradio demo 
============
â€” third revision â€”
â€¢ **ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° history** â€” Ñ‚ĞµĞ¿ĞµÑ€ÑŒ `Chatbot` Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº *Ğ¿Ğ°Ñ€*
  `(user_text, bot_text)`.  Ğ§Ğ°Ğ½ĞºĞ¸ Ğ±Ğ¾Ñ‚Ğ° Ğ°Ğ¿Ğ´ĞµĞ¹â€‘Ñ‚ÑÑ‚ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ Ğ¿Ğ°Ñ€Ñ‹,
  Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Â«Ğ´ÑƒĞ±Ğ»Ğ¸Â» Ğ¸ Â«Ñ€Ğ¾Ğ±Ğ¾Ñ‚â€‘ÑĞ·ĞµÑ€Â» Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‚.  
â€¢ **ĞÑˆĞ¸Ğ±ĞºĞ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ°** Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ÑÑ *ĞºĞ°Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ* Ğ¸ Ğ½ĞµĞ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾
  Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ² Gemini; Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚, Ğ¼Ñ‹ ÑĞ½Ğ¾Ğ²Ğ° Ğ¿Ñ‹Ñ‚Ğ°ĞµĞ¼ÑÑ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´ â€”
  Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ†Ğ¸ĞºĞ», ĞºĞ°Ğº Ğ² Ğ²Ğ°ÑˆĞµĞ¼ CLIâ€‘ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğµ.  
â€¢ Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ ÑĞ²ĞµĞ´ĞµĞ½Ğ¾ Ğº Ñ‡Ñ‘Ñ‚ĞºĞ¸Ğ¼ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼: `await_task`, `coding_loop`,
  `await_feedback`, `finished`.
â€¢ ĞŸĞ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ â€”
  Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ² Gemini Ğ¸ ĞºĞ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ·Ğ°Ğ½Ğ¾Ğ²Ğ¾ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğ¹.

Ğ—Ğ°Ğ¿ÑƒÑĞº:
```bash
pip install --upgrade gradio google-genai manim_video_generator
export GEMINI_API_KEY="YOUR_KEY"
python gradio_manim_gemini_app.py
```
"""
from __future__ import annotations

import asyncio
import os
import re
import traceback
from pathlib import Path
from typing import List, Tuple

import gradio as gr
from google import genai
from google.genai.chats import Chat, AsyncChat
from google.genai.types import GenerateContentConfig, ThinkingConfig, UploadFileConfig

from manim_video_generator.video_executor import VideoExecutor  # type: ignore
from prompts import SYSTEM_PROMPT_SCENARIO_GENERATOR, SYSTEM_PROMPT_CODEGEN

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Config  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise EnvironmentError("GEMINI_API_KEY env variable not set.")

client = genai.Client(api_key=API_KEY)
MODEL = "gemini-2.5-flash-preview-05-20"
video_executor = VideoExecutor()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Helpers to work with Chatbot  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def add_user_msg(history: List[Tuple[str, str]], text: str):
    """Append new (user, Â«Â») pair."""
    history.append((text, ""))


def append_bot_chunk(history: List[Tuple[str, str]], chunk: str):
    """Add chunk to bot part of the last pair."""
    user, bot = history[-1]
    history[-1] = (user, bot + chunk)


class StreamPart:
    def __init__(self, text: str):
        self.text = text

class ThinkingStreamPart(StreamPart): pass
class TextStreamPart(StreamPart): pass


async def stream_parts(chat, prompt):
    cfg = GenerateContentConfig(thinking_config=ThinkingConfig(include_thoughts=True))
    async for chunk in await chat.send_message_stream(prompt, config=cfg):
        if chunk.candidates:
            cand = chunk.candidates[0]
            if cand.content and cand.content.parts:
                for part in cand.content.parts:
                    if part.text:
                        if part.thought:
                            yield ThinkingStreamPart(part.text)
                        else:
                            yield TextStreamPart(part.text)


def extract_python(md: str) -> str:
    m = re.search(r"```python(.*?)```", md, re.S)
    if not m:
        raise ValueError("No ```python``` block found in model output.")
    return m.group(1).strip()


async def coding_cycle(state: "Session", history: List[Tuple[str, str]], prompt):
    """Generate code, render video and return once rendering succeeds."""
    while True:
        async for chunk in stream_parts(state.chat, prompt):
            append_bot_chunk(history, chunk.text)
            yield history, state, state.last_video
            await asyncio.sleep(0)

        full_answer = history[-1][1]
        try:
            py_code = extract_python(full_answer)
        except ValueError as e:
            err_msg = f"Error: {e}. Please wrap the code in ```python``` fence."
            prompt = err_msg
            add_user_msg(history, err_msg)
            yield history, state, state.last_video
            continue

        try:
            video_path = video_executor.execute_manim_code(py_code)
            state.last_video = video_path
        except Exception as e:
            tb = traceback.format_exc(limit=10)
            err_msg = (
                f"Error, your code is not valid: {e}. Traceback: {tb}. Please fix this error and regenerate the code again."
            )
            prompt = err_msg
            add_user_msg(history, err_msg)
            yield history, state, state.last_video
            continue

        append_bot_chunk(history, "\nğŸï¸ Rendering done! Feel free to request changes or type **finish** to end.")
        state.phase = "await_feedback"
        yield history, state, state.last_video
        return

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Session state  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Session(dict):
    phase: str  # await_task | coding_loop | await_feedback | finished
    chat: AsyncChat | None
    last_video: Path | None

    def __init__(self):
        super().__init__(phase="await_task", chat=None, last_video=None)
        self.phase = "await_task"
        self.chat = None
        self.last_video = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Main chat handler  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def chat_handler(user_msg: str, history: List[Tuple[str, str]], state: Session):
    history = history or []

    # 0. Always reflect user input
    add_user_msg(history, user_msg)
    yield history, state, state.last_video

    # bootstrap chat on very first user request
    if state.phase == "await_task":
        if not state.chat:
            # First time - create chat and generate scenario
            state.chat = client.aio.chats.create(model=MODEL)
            scenario_prompt = f"{SYSTEM_PROMPT_SCENARIO_GENERATOR}\n\n{user_msg}"
            async for txt in stream_parts(state.chat, scenario_prompt):
                append_bot_chunk(history, txt.text)
                yield history, state, state.last_video
                await asyncio.sleep(0)
            append_bot_chunk(history, "\n\n*(type **continue** to proceed to code generation)*")
            yield history, state, state.last_video
            return
        else:
            # Chat exists - check if user wants to proceed or modify scenario
            if user_msg.strip().lower() in {"c", "continue", "Ñ"}:
                # User is ready to proceed to code generation
                state.phase = "coding_loop"
            else:
                # User wants to discuss/modify scenario
                async for chunk in stream_parts(state.chat, user_msg):
                    append_bot_chunk(history, chunk.text)
                    yield history, state, state.last_video
                    await asyncio.sleep(0)
                append_bot_chunk(history, "\n\n*(type **continue** when ready to proceed to code generation)*")
                yield history, state, state.last_video
                return

    # later phases require chat obj
    if not state.chat:
        raise ValueError("Chat not found")

    # â”€â”€ Coding loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if state.phase == "coding_loop":
        prompt = "Thanks. It is good scenario. Now generate code for it.\n\n" + SYSTEM_PROMPT_CODEGEN
        async for out in coding_cycle(state, history, prompt):
            yield out
        return
    # â”€â”€ Awaiting user feedback after rendering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if state.phase == "await_feedback":
        if user_msg.strip().lower() in {"finish", "done", "f"}:
            state.phase = "finished"
            append_bot_chunk(history, "Session complete. Refresh page to start over.")
            yield history, state, state.last_video
            return
        file_ref = client.files.upload(file=state.last_video, config=UploadFileConfig(display_name=state.last_video.name))
        while file_ref.state and file_ref.state.name == "PROCESSING":
            await asyncio.sleep(3)
            if file_ref.name:
                file_ref = client.files.get(name=file_ref.name)
        if file_ref.state and file_ref.state.name == "FAILED":
            raise RuntimeError("Gemini failed to process upload")
        prompt = [file_ref, f"{user_msg}\n\n{SYSTEM_PROMPT_CODEGEN}"]
        state.phase = "coding_loop"
        async for out in coding_cycle(state, history, prompt):
            yield out
        return

    # â”€â”€ Finished phase â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if state.phase == "finished":
        append_bot_chunk(history, "Session complete. Refresh page to start over.")
        yield history, state, state.last_video

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  UI  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_app():
    with gr.Blocks(title="Geminiâ€‘Manim Video Creator") as demo:
        gr.Markdown("# ğŸ¬ Geminiâ€‘Manim Video Creator\nCreate an explanatory animation from a single prompt.")

        history = gr.Chatbot(height=850)
        session = gr.State(Session())

        with gr.Row():
            txt = gr.Textbox(placeholder="Describe the conceptâ€¦", scale=4)
            btn = gr.Button("Send", variant="primary")

        vid = gr.Video(label="Rendered video", interactive=False)

        def get_vid(state: Session):
            return state.last_video if state.last_video else None

        btn.click(chat_handler, [txt, history, session], [history, session, vid]) \
           .then(lambda: "", None, txt)

    return demo


if __name__ == "__main__":
    build_app().launch()
